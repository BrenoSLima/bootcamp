{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOs5AbqlSKPqLqf+FtAXcbP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Proposta da tarefa\n","\n","Como entender e manipular dados do mundo real ao lidar com projetos de Machine Learning. Diversos conceitos e ferramentas de data science são introduzidos nessa tarefa.\n","\n","Curso: [Machine Learning, Data Science and Generative AI with Python](https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/?couponCode=ST7MT41824)\n","\n","\n","Seções: More Data Mining and Machine Learning Techniques; Dealing with Real-World Data. (2 horas e 30 minutos)\n"],"metadata":{"id":"zs8HDHltff4T"}},{"cell_type":"markdown","source":["\n","\n","# K - Nearest Neighbor (KNN)\n","\n","\n","Classificador baseado na distância de um novo dado com base na categoria de seus vizinhos. É considerada a categoria dos k vizinhos mais próximos, para votar qual categoria deverá ser classificada ao novo dado.\n","\n","\n","## Usando KNN para prever nota de um filme\n","\n","\n","A partir de informações como notas e categorias de filmes, ambas já conhecidas e fornecidas, cria-se uma métrica de distância para se usar o KNN. Essa métrica é feita com base nos gêneros que cada filme se enquadra, e então o quão próximo um está de outro é calculado a partir dos gêneros dos filmes.\n","\n","\n","O data set fornecido possui identificação de usuário, identificação dos filmes e suas notas.\n","\n","\n","A partir dessas informações, groupby pode ser utilizado para agrupar os filmes pelas suas ids. Então, de todas as votações para determinados filmes, calcula-se quantas pessoas votaram em um filme e a média dessas votações.\n","\n","\n","O mesmo é feito através do seguinte comando agg:\n","\n","\n","\n"],"metadata":{"id":"3uekHlfMSc_H"}},{"cell_type":"code","source":["movieProperties = ratings.groupby('movie_id').agg('rating': [np.size, np.mean])"],"metadata":{"id":"CBkiu3axWiWd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Após isso, entende-se que a popularidade de um filme sendo medida através do número de votações feita nele não é uma aproximação boa para entender a sua popularidade, pois um número sozinho pode ter sua interpretação variada em diferentes contextos. Para isso, é normalizado o número de votações em um filme em um intervalo de 0 até 1, sendo aqueles filmes próximos de 1 filmes mais populares, ou seja, com mais votos.\n","\n","\n","Esse entendimento é algo subjetivo e cabe ao profissional tê-lo, algo importante a se notar para casos futuros.\n","\n","\n","Obtendo em mãos esse cenário de filmes com suas popularidades e notas, a nota de um novo filme pode ser calculada através da médias das notas de seus k vizinhos mais próximos (gêneros similares).\n","\n","\n","KNN se mostra uma alternativa boa e simples para a predição de recomendações de filmes no contexto do curso.\n","\n","\n","# Maldição da dimensionalidade\n","\n","\n","A maldição da dimensionalidade refere-se ao desafio enfrentado quando lidamos com conjuntos de dados de alta dimensionalidade. À medida que o número de características ou dimensões aumenta, o consumo de recursos computacionais e dificuldade em encontrar padrões significativos nos dados também aumenta. Para combater essa maldição, é importante realizar seleção de características e redução de dimensionalidade a fim de melhorar o desempenho dos modelos de machine learning.\n","\n","\n","## PCA\n","\n","\n","Uma das soluções propostas para esse problema é a Principal Component Analysis, uma abordagem que aplica algebra linear em dimensões superiores criando planos com maiores variações e projetando dados em dimensões menores reduzindo o número de colunas.\n","\n","\n","O uso dessa técnica é mostrada no dataset íris, onde através do objeto criado para o PCA é possível ver quanto da variância ainda está presente no dataset com as colunas reduzidas.\n","\n","\n","Uma das razões para aplicar o PCA é possibilitar a plotagem dos dados em duas dimensões, selecionando os 2 componentes principais.\n","\n","\n","# Data Warehousing\n","\n","\n","Uma base de dados centralizada, contendo informações de diferentes fontes, e portanto possuindo um grande volume de dados, é um data warehouse. Um dos seus objetivos é possibilitar análise de dados de grandes corporações.\n","\n","\n","Esse cenário promove diferentes desafios, como dados ausentes, normalização de dados e como juntar e manter dados de diferentes fontes. Com um volume de dados grande, as próprias transformações nos dados podem se transformar também em um problema.\n","\n","\n","Para a criação e manutenção de um Data Warehouse existem duas formas:\n","\n","\n","## ETL (extrair, transformar e carregar)\n","\n","\n","A ETL indica uma ordem de processos para o Data Warehousing que é mais convencional, extraindo dados do mundo real, transformando eles no padrão desejado, e então carregados ao warehouse.\n","\n","\n","Essa abordagem possui um problema de escala. Visto que as transformações são aplicadas em todos os dados a fim de padronizá-los, quando se há um grande volume de dados, essa passa a ser uma tarefa difícil por si só.\n","\n","\n","## ELT (extrair, carregar e transformar)\n","\n","\n","Diferente do método anterior, ELT extrai os dados e logo em seguida os carrega, onde o próprio poder computacional da warehouse irá aplicar as transformações nos dados. Esse processo de transformação é feito através de ferramentas citadas no vídeo como hadoop e hive, as quais permitem que seja possível dar query nos dados.\n","\n","\n","# Reinforcement Learning\n","\n","\n","Uma das táticas de aprendizado é envolver um agente que explora um ambiente, e premiá-lo com ações corretas, reforçando um certo comportamento.\n","\n","\n","## Q-Learning\n","\n","\n","Permite que um agente aprenda a tomar decisões sequenciais em um ambiente, otimizando suas ações para maximizar uma recompensa cumulativa ao longo do tempo. No caso do vídeo é mostrado o exemplo do pacman, onde é armazenado as possíveis decisões que o boneco pode fazer, iniciando cada uma com qualidade 0. O agente atualiza esses valores com base nas recompensas observadas, permitindo que ele aprenda a melhor estratégia para alcançar seus objetivos ao longo do tempo.\n","\n","\n","Um dos problemas para essa abordagem é a falta de eficiência em explorar todas as possibilidades de aprendizado.\n","\n","\n","# Matriz de confusão\n","\n","\n","Ferramenta fundamental na avaliação de modelos de classificação. Ela é uma tabela que permite comparar as previsões de um modelo com os valores reais dos dados. Essa matriz fornece uma visão clara do desempenho do modelo, permitindo calcular métricas importantes, como precisão, recall, F1-score e a taxa de acerto, que são cruciais para avaliar a qualidade de um algoritmo de classificação.\n","\n","\n","## Precisão (Precision)\n","\n","\n","A precisão mede a proporção de instâncias positivas previstas corretamente em relação ao total de instâncias previstas como positivas. É uma métrica que avalia a capacidade do modelo de evitar classificar erroneamente negativos como positivos.\n","\n","\n","## Recall:\n","\n","\n","O recall avalia a capacidade do modelo de encontrar corretamente todas as instâncias positivas. Ele mede a proporção de instâncias positivas reais que foram identificadas corretamente pelo modelo.\n","\n","\n","## F1-Score:\n","\n","\n","O F1-Score é uma métrica que combina precisão e recall em um único valor. É útil quando é necessário equilibrar a importância de evitar falsos positivos e falsos negativos. Quanto maior o F1-Score, melhor o equilíbrio entre precisão e recall.\n","\n","\n","## Curva ROC (Receiver Operating Characteristic)\n","\n","\n","A curva ROC é uma representação gráfica da capacidade discriminativa de um modelo em diferentes limiares de classificação. Ela mostra a taxa de verdadeiros positivos em relação à taxa de falsos positivos.\n","\n","\n","## AUC (Area Under the Curve)\n","\n","\n","A AUC é a área sob a curva ROC e fornece uma medida quantitativa da capacidade de diferenciação do modelo entre classes. Quanto maior a AUC, melhor o desempenho do modelo em classificar as instâncias corretamente.\n","\n","\n","# Bias e Variance\n","\n","\n","Bias mede o quão distante os dados estão dos valores corretos. Um dados com Bias são dados consistentes pendendo para um lado.\n","\n","\n","Variância diz respeito a quão espalhados os dados estão.\n","\n","\n","Sendo assim, qualquer conjunto de dados tem ambos em diferentes quantidades.\n","\n","\n","## Erro\n","\n","\n","Ao fim do dia, o objetivo se torna diminuir o erro, o qual é medido através do bias e variância.\n","\n","\n","Erro = bias^2 + variância\n","\n","\n","Esses termos podem descrever comportamentos de modelos, e é uma forma forma de expressar informações sobre diferentes casos reais.\n","\n","\n","# K-fold cross validation\n","\n","\n","Normalmente os modelos são treinados a partir dos dados de treinos, para então serem avaliados nos dados de testes. O problema presente nesse cenário é o modelo ser avaliado erroneamente, pois o modelo pode se adaptar a dados de treinos que não representam totalmente o problema.\n","\n","\n","Ao invés disso, cross validation propõe separar os dados em diferentes pedaços de tamanhos iguais. A partir disso, o modelo irá aprender e ser testado com partes diferentes dos dados, trazendo uma perspectiva mais ampla de como ele performa nos dados.\n","\n","\n","# Limpando os dados de entrada\n","\n","\n","Limpar os dados reflete diretamente no restante da pipeline de um problema, influenciado pelo resultado de modelos treinados nela, portanto essa é uma parte importante do processo.\n","\n","\n","##Outliers\n","\n","\n","Valores atípicos que se destacam em um conjunto de dados, podendo distorcer análises e modelos.\n","\n","\n","## Dados faltantes\n","\n","\n","A presença de dados faltantes é comum em conjuntos de dados reais. Tratá-los envolve imputação (estimar valores ausentes) ou exclusão de observações, dependendo do impacto nos resultados.\n","\n","\n","## Dados maliciosos\n","\n","\n","Dados maliciosos são inseridos para enganar ou corromper análises e modelos.\n","\n","\n","## Dados errôneos\n","\n","\n","Dados errôneos são informações imprecisas ou incorretas, resultantes de erros humanos ou de medição.\n","\n","\n","## Dados irrelevantes\n","\n","\n","Dados irrelevantes não contribuem para os objetivos da análise ou modelo.\n","\n","\n","## Dados inconsistentes\n","\n","\n","Inconsistências ocorrem quando os dados contradizem-se ou não seguem padrões esperados.\n","\n","\n","## Formatação\n","\n","\n","A formatação envolve padronizar valores e tipos de dados, facilitando a análise e a compreensão, garantindo que datas, números e categorias estejam corretos.\n","\n","\n","# Normalizando dados\n","\n","\n","Garante que as informações estejam em um formato uniforme e comparável. Isso é fundamental para que alguns algoritmos e modelos possam operar eficazmente, evitando distorções decorrentes de diferentes escalas, unidades ou representações de dados.\n","\n","\n","Há a possibilidade de desnormalizar os dados ao fim para interpretá-los.\n","\n","\n","# Outliers\n","\n","\n","Valores discrepantes e atípicos. Devem ser analisados e julgados para serem desconsiderados ou não, pois há cenários onde podem ser positivos.\n","\n","\n","# Feature Engineering\n","\n","\n","Envolve a criação, transformação e seleção de variáveis (características) a partir dos dados brutos. A importância da feature engineering reside no fato de que as características adequadas podem melhorar significativamente o desempenho dos modelos, tornando-os mais capazes de capturar relações complexas nos dados. Isso envolve a identificação de características informativas, a redução de dimensionalidade e a criação de representações mais significativas dos dados.\n","\n","\n","# Inputing missing data\n","\n","\n","Uma das formas de tratar dados faltantes é subtítulos pela média da coluna. Esse caso é afetado quando há outliers nos dados, tornando a opção da mediana uma opção melhor.\n","\n","\n","Uma das melhores formas é usar modelos de machine learning para predizer os valores dos dados faltantes. Isso serve bem para dados numéricos pois cria-se uma medição de distância entre os dados.\n","\n","\n","Para dados faltantes categóricos onde não há como criar essa distância, deep learning são mostrados como modelos mais adequados.\n","\n","\n","No fim, quando se há dados faltantes, a melhor opção será coletar mais dados.\n","\n","\n","# Dados desbalanceados\n","\n","\n","Dados desbalanceados ocorrem quando as classes em um conjunto de dados não estão igualmente representadas. Isso pode levar a problemas, pois modelos de aprendizado tendem a favorecer a classe majoritária, resultando em previsões enviesadas e desempenho insatisfatório nas classes minoritárias.\n","\n","\n","## Oversampling\n","\n","\n","Envolve a criação de cópias adicionais de observações da classe minoritária para igualar o número de amostras em ambas as classes. Isso ajuda a evitar o viés em direção à classe majoritária e a melhorar o desempenho do modelo nas classes minoritárias.\n","\n","\n","## Undersampling\n","\n","\n","Algumas amostras da classe majoritária são aleatoriamente removidas para equilibrar as proporções entre as classes. Isso reduz a influência da classe majoritária e pode melhorar a capacidade do modelo de identificar a classe minoritária.\n","\n","\n","Essa abordagem pode não fazer muito sentido na maioria dos casos, visto que ela propõe jogar dados fora. Uma possibilidade de cenário para se descartar dados, é talvez quando não se há poder computacional o suficiente para processar um grande volume.\n","\n","\n","## SMOTE\n","\n","\n","O SMOTE (Synthetic Minority Over-sampling Technique) é uma técnica de oversampling que cria novas amostras sintéticas para a classe minoritária, gerando pontos intermediários entre instâncias vizinhas.\n","\n","\n","# Outras técnicas de feature engineering\n","\n","\n","## Binning\n","\n","\n","Técnica que envolve a divisão de um conjunto de dados em intervalos (bins) com base em valores numéricos ou categorias. Isso pode ser útil para simplificar dados contínuos, criando intervalos discretos e facilitando a análise ou a visualização de padrões nos dados.\n","\n","\n","## Transforming\n","\n","\n","Processos que modificam a estrutura ou o formato dos dados. Isso pode incluir operações como logaritmo, exponenciação ou aplicação de funções matemáticas para tornar os dados mais adequados para análises estatísticas ou para atender aos requisitos de determinados modelos de aprendizado de máquina.\n","\n","\n","## Encoding\n","\n","\n","A codificação é o processo de converter dados categóricos em uma forma numérica, permitindo que algoritmos de aprendizado de máquina trabalhem com esses dados. Existem técnicas de codificação, como one-hot encoding e label encoding, que são usadas para representar categorias de maneira apropriada para análise ou modelagem.\n","\n","\n","## Scaling\n","\n","\n","O dimensionamento é o processo de ajustar a escala dos dados, geralmente normalizando ou padronizando os valores. Isso é importante para garantir que diferentes características ou variáveis tenham a mesma influência em modelos de aprendizado de máquina, evitando que uma característica domine as outras devido a diferentes escalas.\n","\n","\n","## Shuffling\n","\n","\n","Shuffling envolve a reorganização aleatória das observações ou amostras em um conjunto de dados. Isso é frequentemente usado durante a preparação de dados ou treinamento de modelos para garantir que as amostras não sigam uma ordem específica que possa introduzir viés ou padrões indesejados na análise ou no modelo.\n","\n","\n","\n"],"metadata":{"id":"Mpx-3Y3BW97L"}}]}