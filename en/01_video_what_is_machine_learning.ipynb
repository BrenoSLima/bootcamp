{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task proposal\n",
        "\n",
        "Through two videos, introduce the concepts of machine learning: Universal approximation theorem, Deep Networks, Backpropagation and Convolutional neural networks. The first being a video from the Nerdologia channel, and the second from the tensorflow course.\n",
        "\n",
        "Video 1: [Neural Networks and Machine Learning | Nerdologia Tech](https://youtu.be/1_c_MA1F-vU?si=yS8i35a5K81os_oS) (9:12 minutes)\n",
        "\n",
        "Video 2: [Introduction to Machine Learning (ML from Zero to 100, part 1)](https://youtu.be/t5z5lyrb-7s?si=2Q9Wu5f85hRZ6lNd) (6:50 minutes)"
      ],
      "metadata": {
        "id": "aB8kRWmLVsCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video 1: Neural Networks and Machine Learning\n",
        "\n",
        "In the nerdologia video, an analogy is made by associating a neural network with a group of friends, who together vote on the value of a product sold in a valuable items store. Consulting these groups of friends is equivalent to a connected layer in a neural network. In this analogy, the person selling the items does not know the group exists, which makes the friends layer a hidden layer.\n",
        "\n",
        "Different friends have different weights, for example, a friend who specializes in art will know how to judge a painting better than a friend who specializes in antiques. These friends can also give their opinion many times and get it right a few times, just as they can also give their opinion a little and get it right a lot of times, which characterizes the activating and inhibitory functions.\n",
        "\n",
        "As new items are purchased, you learn how much weight to give each opinion from that group of friends, and that is the learning process.\n",
        "\n",
        "## Universal approximation theorem\n",
        "\n",
        "A network with many layers may encode the best decisions to solve a problem, however, this network may be too large or require too much training. In the example above it would be equivalent to having a group of 5000 friends or training models for thousands of years. When it comes to universal approximation, it is crucial to consider the balance between complexity and efficiency, ensuring that the network is capable of capturing relevant patterns without becoming impractical in terms of computational resources and training time.\n",
        "\n",
        "## Deep networks\n",
        "\n",
        "One way to solve the universal approximation theorem is to create a network that, despite having few neurons, will be organized into several different layers. This allows the network to capture nuances and complexities of data, even with a relatively low number of processing units at each layer. Thus, the depth of the network, with its multiple layers, facilitates the representation of complex functions, keeping computational efficiency and training time under control.\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "To be able to use errors to improve the neural network's decisions, backpropagation can be used as a form of learning. During training, errors are propagated back through the network, allowing each layer to adjust its weights according to its contribution to the total error, which makes it possible to minimize loss.\n",
        "\n",
        "## Convolutional neural networks\n",
        "\n",
        "They use specialized layers to identify the most important patterns in the data provided. These convolutional layers apply filters to detect specific features, such as edges, textures, and shapes, in different regions of the input data. This makes them especially suitable for computer vision tasks such as image recognition and object detection.\n",
        "\n",
        "# Video 2: Introduction to Machine Learning (ML from Zero to 100, part 1)\n",
        "\n",
        "## Example of rock, paper, scissors\n",
        "\n",
        "In the tensorflow introductory video, the example of the game of rock, paper, scissors is cited as a way of exemplifying a machine learning project, as well as demonstrating the difficulties encountered by a model when learning, which in this case must learn hand patterns of different colors, sizes, each with different symbols for each item in the game. The computer would need to be able to tell if a hand is in the shape of scissors regardless of the person's skin, nails, and hand position.\n",
        "\n",
        "The program that will solve the problem will learn from answers already obtained from the data, to then build the rules that govern the game, and this is machine learning.\n",
        "\n",
        "In the video, an example is also created where the network must approximate a function that tells the value of Y given an input X, where the data is:\n",
        "\n",
        "X: -1 0 1 2 3 4\n",
        "\n",
        "Y: -2 1 4 7 10 13\n",
        "\n",
        "The actual function would be:\n",
        "\n",
        "f(x) = 3x+1\n",
        "\n",
        "And the code shown in the video is executed below, where once learned from the data, the model tries to predict the value for the function when X=10, where the expected result is 31, given the real function that the model will approximate."
      ],
      "metadata": {
        "id": "S6AKfMyCuSu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=100, verbose=0)\n",
        "\n",
        "print(model.predict([10.0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzWUpdK795Gk",
        "outputId": "bd629c46-22a3-4e5e-bd79-1a10c1d44009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 156ms/step\n",
            "[[31.23818]]\n"
          ]
        }
      ]
    }
  ]
}